{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a565264",
   "metadata": {},
   "source": [
    "## Compute monthly sea-ice concentration climatologies from EUMETSAT OSI SAF data\n",
    "\n",
    "We compute monthly cliamtologies (1981-2010, 1991-2020) from EUMETSAT OSI SAF SIC CDR v3 data. This uses xarray.\n",
    "\n",
    "**NB :** This software is for demonstration purpose only, it is not finalized for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2724be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "from datetime import date, datetime, timedelta, time\n",
    "from dateutil import rrule, relativedelta\n",
    "\n",
    "import cftime\n",
    "\n",
    "import json\n",
    "import uuid\n",
    "from copy import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db3e79c",
   "metadata": {},
   "source": [
    "Configuration of the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "000192ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "area = 'nh'\n",
    "outdir = '.'\n",
    "indirs = 'osisaf_sic_cdr_storeB.json'\n",
    "\n",
    "# climo : years to include in the climatology run. **Both ends are included**.\n",
    "climo = (1991, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59061b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input monthly SIC files\n",
    "fn_patt = 'ice_conc_{a:}_ease2-250_{c:}_{d:%Y%m}.nc'\n",
    "fn_patt_src = {'cdr': 'cdr-v3p0', 'icdr': 'icdr-v3p0', 'icdrft': 'icdrft-v3p0'}\n",
    "\n",
    "if indirs is None:\n",
    "    inpdir_cdr = 'https://thredds.met.no/thredds/dodsC/osisaf/met.no/reprocessed/ice/conc_450a_files/monthly/'\n",
    "    inpdir_icdr = 'https://thredds.met.no/thredds/dodsC/osisaf/met.no/reprocessed/ice/conc_cra_files/monthly/'\n",
    "    # access through THREDDS/OpenDAP\n",
    "    sources = {'cdr':inpdir_cdr,\n",
    "              'icdr':inpdir_icdr,\n",
    "              'icdrft':inpdir_icdr}\n",
    "    jsond = json.dumps(sources, sort_keys=True, indent=4)\n",
    "else:\n",
    "    # load json file with path to input directories (if the daily SIC files are downloaded to a local disk)\n",
    "    # an example json file (prepare_monthly_osisaf_sic_opendap.json) is provided to demonstrate the format\n",
    "    #   expected for the json file (but the effect will be the same as setting indirs to None: read from\n",
    "    #   THREDDS/opendap)\n",
    "    with open(indirs, 'r') as f:\n",
    "        sources = json.load(f)\n",
    "        for s in sources.keys():\n",
    "            sources[s] += '/monthly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff91ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open 120 monthly files:\n",
      "/lustre/storeB/project/copernicus/osisaf/data/reprocessed/ice/conc/v3p0//monthly/1991/ice_conc_nh_ease2-250_cdr-v3p0_199101.nc\n",
      " ... \n",
      " ... \n",
      "/lustre/storeB/project/copernicus/osisaf/data/reprocessed/ice/conc/v3p0//monthly/2000/ice_conc_nh_ease2-250_cdr-v3p0_200012.nc\n"
     ]
    }
   ],
   "source": [
    "def find_one_monthly_sic_file(dt, area):\n",
    "    \n",
    "    found_one_file = False\n",
    "    for cdr in ('cdr', 'icdr', 'icdrft'):\n",
    "        fn = fn_patt.format(a=area, d=dt, c=fn_patt_src[cdr])\n",
    "        fn = os.path.join(sources[cdr],'{:%Y/}'.format(dt),fn)\n",
    "        try:\n",
    "            # this url exists, append it and move to next date\n",
    "            ds = xr.open_dataset(fn)\n",
    "            found_one_file = True\n",
    "            return fn, cdr\n",
    "        except OSError:\n",
    "            # no valid file at this url, check the next rule\n",
    "            # print(\"Failed with pattern {}\".format(fn))\n",
    "            pass\n",
    "        \n",
    "    # no file found. Add a warning (but we can continue)\n",
    "    if not found_one_file:\n",
    "        print(\"WARNING: could not find OSI SAF SIC v3 file for {} {}\".format(area, dt))\n",
    "        return None, None\n",
    "\n",
    "def find_climo_monthly_sic_file(year_start, year_end, area):\n",
    "    \n",
    "    files = []\n",
    "    for year in range(year_start, year_end+1):\n",
    "        for month in range(1,13):\n",
    "            dt = date(year, month, 15)\n",
    "            fn, src = find_one_monthly_sic_file(dt, area)\n",
    "            if fn:\n",
    "                files.append(fn)\n",
    "    \n",
    "    return files\n",
    "\n",
    "climo_fns = find_climo_monthly_sic_file(climo[0], climo[1], area,)\n",
    "if len(climo_fns) == 0:\n",
    "    raise ValueError(\"Found no files for building the climatology by month.\")\n",
    "\n",
    "print(\"Open {} monthly files:\".format(len(climo_fns)))\n",
    "print(climo_fns[0])\n",
    "print(' ... ')\n",
    "print(' ... ')\n",
    "print(climo_fns[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "382a766d-856c-4eb5-8d47-c97c92735060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open one of the original files to extract information like dtype, _FillValue, time units, etc...\n",
    "dso = xr.open_dataset(climo_fns[0],mask_and_scale=False,decode_times=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2105ae",
   "metadata": {},
   "source": [
    "## Climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbdb63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A routine to reconstruct the unfiltered, unthresholded SICs from a OSI SAF SIC CDR file\n",
    "def reconstruct_sic(ds):\n",
    "    ice_conc = ds['ice_conc'].to_masked_array()\n",
    "    raw_ice_conc_values = ds['raw_ice_conc_values'].to_masked_array()\n",
    "    status_flag = ds['status_flag'].to_masked_array().astype('short')\n",
    "    \n",
    "    # combine ice_conc with raw_ice_conc_values using the status_flag\n",
    "    new_ice_conc = copy(ice_conc)\n",
    "    raw_100_mask = np.array((ice_conc==100) * (~raw_ice_conc_values.mask))\n",
    "    new_ice_conc[raw_100_mask] = raw_ice_conc_values[raw_100_mask]\n",
    "    \n",
    "    new_ice_conc[(status_flag & 4) == 4] = raw_ice_conc_values[(status_flag & 4) == 4]\n",
    "\n",
    "    # re-enter \"full\" ice_conc into the xarray dataset\n",
    "    ds['ice_conc'][:] = new_ice_conc\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d22d531f-f9f6-4006-9df4-f617b3393ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open all the monthly files for the climatology period\n",
    "climo_ds = xr.open_mfdataset(climo_fns, engine='netcdf4')\n",
    "climo_ds = reconstruct_sic(climo_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fb39c8d-760e-4348-99ad-1982daf80ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some variablea and attributes before computing the climatology\n",
    "climo_ds = climo_ds.drop_vars(['lat', 'lon', 'raw_ice_conc_values', 'status_flag', 'Lambert_Azimuthal_Grid', 'time_bnds'])\n",
    "\n",
    "del climo_ds['ice_conc'].attrs['valid_min']\n",
    "del climo_ds['ice_conc'].attrs['valid_max']\n",
    "del climo_ds['ice_conc'].attrs['ancillary_variables']\n",
    "del climo_ds['ice_conc'].attrs['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8a01bdc-10ba-4714-b807-e6c602e21ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the monthly fields by month, and compute the statistics\n",
    "climo_ds = climo_ds.chunk({'time': -1})\n",
    "climo_grpby = climo_ds.groupby('time.month')\n",
    "\n",
    "# compute mean, sdev, and all quantiles (include median, min, and max)\n",
    "quantiles = np.array([50, 0, 100, 5, 10, 25, 75, 90, 95])/100.\n",
    "ds_mean = climo_grpby.mean(skipna=True, keep_attrs=True)\n",
    "ds_mean = ds_mean.rename_vars({'ice_conc':'ice_conc_mean'})\n",
    "ds_mean['ice_conc_mean'].attrs['cell_methods'] = 'time: mean within years time: mean over years'\n",
    "ds_mean['ice_conc_mean'].attrs['long_name'] = 'mean sea-ice concentration over the climalogical period'\n",
    "\n",
    "ds_sdev = climo_grpby.std(skipna=True, keep_attrs=True)\n",
    "ds_sdev = ds_sdev.rename_vars({'ice_conc':'ice_conc_sdev'})\n",
    "ds_sdev['ice_conc_sdev'].attrs['cell_methods'] = 'time: mean within years time: standard_deviation over years'\n",
    "ds_sdev['ice_conc_sdev'].attrs['long_name'] = 'standard deviation of sea-ice concentration over the climalogical period'\n",
    "\n",
    "ds_climo = xr.merge([ds_mean, ds_sdev], compat='override')\n",
    "\n",
    "ds_quantiles = climo_grpby.quantile(quantiles, skipna=True, keep_attrs=True)\n",
    "for q in quantiles:\n",
    "    lname = None\n",
    "    if q == 0:\n",
    "        suff = 'min'\n",
    "        meth = 'minimum'\n",
    "    elif q == 1:\n",
    "        suff = 'max'\n",
    "        meth = 'maximum'\n",
    "    elif q == 0.5:\n",
    "        suff = 'median'\n",
    "        meth = 'median'\n",
    "    else:\n",
    "        qpct = round(q*100)\n",
    "        suff = '{:d}pctile'.format(qpct)\n",
    "        meth = 'percentile_{:d}'.format(qpct)\n",
    "        lname = '{}% Percentile'.format(qpct)\n",
    "\n",
    "    if lname is None:\n",
    "        lname = meth.capitalize()\n",
    "                    \n",
    "    ds_quant = ds_quantiles.sel(quantile=q)\n",
    "    ds_quant = ds_quant.rename_vars({'ice_conc':'ice_conc_'+suff})\n",
    "    ds_quant['ice_conc_'+suff].attrs['cell_methods'] = 'time: mean within years time: {} over years'.format(meth,)\n",
    "    ds_quant['ice_conc_'+suff].attrs['long_name'] = lname + ' of sea-ice concentration over the climalogical period'\n",
    "    \n",
    "    ds_climo = xr.merge([ds_climo, ds_quant], compat='override')\n",
    "\n",
    "ds_climo = ds_climo.drop_vars(['quantile',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f91ec9d5-21f1-4531-b551-4c158ba1ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the time coordinate\n",
    "mid_year = int(round(0.5 * (climo[0] + climo[1])))\n",
    "mid_time = [cftime.datetime(mid_year,m,16) for m in range(1,13)]\n",
    "time_attrs = dso['time'].attrs.copy()\n",
    "for a in dso['time'].attrs.keys():\n",
    "    if a.startswith('_') or a in ('bounds', 'units', 'calendar'):\n",
    "        del time_attrs[a]\n",
    "mid_time_da = xr.DataArray(mid_time, dims=\"month\", coords={'month': range(1,13)}, name='time',)\n",
    "\n",
    "# Assign the time dimension and swap the dimension from 'month' to 'time'\n",
    "ds_climo = ds_climo.assign_coords(time=mid_time_da)\n",
    "ds_climo = ds_climo.swap_dims({\"month\": \"time\"})\n",
    "ds_climo = ds_climo.drop_vars(('month',))\n",
    "\n",
    "# add climatological time bounds\n",
    "bound0 = [cftime.datetime(climo[0],m,1) for m in range(1,13)]\n",
    "bound1 = [cftime.datetime(climo[1],m+1,1) for m in range(1,12)] + [cftime.datetime(climo[1]+1,1,1),]            \n",
    "bounds = np.column_stack([bound0, bound1])\n",
    "bounds_da = xr.DataArray(bounds, dims=['time','nv'], coords={'time': mid_time, },)\n",
    "ds_climo = ds_climo.assign(climatology_bnds=bounds_da)\n",
    "\n",
    "# fix attributes of the time variable\n",
    "time_attrs['climatology'] = 'climatology_bnds'\n",
    "ds_climo['time'].attrs = time_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bda9dc6-1516-41dd-9357-c81aec6d569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer some variables back from the original files\n",
    "ds_climo = ds_climo.assign_coords(lat=dso['lat'])\n",
    "ds_climo = ds_climo.assign_coords(lon=dso['lon'])\n",
    "ds_climo = ds_climo.assign(Lambert_Azimuthal_Grid = dso['Lambert_Azimuthal_Grid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8850a9b5-2f1a-4bf6-a48e-549a6516b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip the ice_conc variables to 0 - 100%\n",
    "for v in ds_climo.variables:\n",
    "    if v.startswith('ice_conc_'):\n",
    "        ds_climo[v] = ds_climo[v].clip(min=0, max=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77034e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add x and y bounds. This should not be necessary here, these bounds should already have been present\n",
    "#   in the monthly sea-ice concentration files\n",
    "for ac in ('xc', 'yc'):\n",
    "    diff = ds_climo[ac].values[1] - ds_climo[ac].values[0]\n",
    "    resol = abs(diff)\n",
    "    sign = diff / resol\n",
    "    b0 = ds_climo[ac].values - 0.5*sign*resol\n",
    "    b1 = ds_climo[ac].values + 0.5*sign*resol\n",
    "    bnds = np.column_stack((b0,b1))\n",
    "    ds_climo[ac + '_bnds'] = xr.DataArray(bnds, dims=(ac,'nv'))\n",
    "    ds_climo[ac].attrs['bounds'] = ac + '_bnds'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd8e626",
   "metadata": {},
   "source": [
    "### Write Climatology to a netCDF file\n",
    "\n",
    "Xarray datasets can be written to netCDF files with the to_netcdf() method.\n",
    "\n",
    "**Note: this is still preliminary format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b956aa64-2f9f-43d7-b902-513c4674b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_last_date_climo(climo, dt):\n",
    "    first_day = date(climo[0],dt.month,1)\n",
    "    last_day = date(climo[1],dt.month,1) + relativedelta.relativedelta(months=1)-timedelta(days=1)\n",
    "    return first_day, last_day\n",
    "\n",
    "def get_first_last_date_month(dt):\n",
    "    first_day = date(dt.year,dt.month,1)\n",
    "    last_day = first_day + relativedelta.relativedelta(months=1)-timedelta(days=1)\n",
    "    return first_day, last_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af6c4ecf-056a-4b0f-b02e-ec1cd0ccb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amend some attributes\n",
    "now = datetime.utcnow().replace(microsecond=0)\n",
    "ds_climo.attrs['history'] = now.isoformat()+'Z' + ' creation'\n",
    "ds_climo.attrs['date_created'] = now.date().isoformat()\n",
    "\n",
    "ds_climo.attrs['tracking_id'] = str(uuid.uuid4())\n",
    "\n",
    "if ds_climo.attrs['title'].startswith('Monthly'):\n",
    "    ds_climo.attrs['title'] = ds_climo.attrs['title'].replace('Monthly','Monthly Climatology ({}-{}) of'.format(climo[0], climo[1]))\n",
    "\n",
    "ds_climo.attrs['product_status'] = 'under development'\n",
    "\n",
    "try:\n",
    "    del ds_climo.attrs['doi']\n",
    "    del ds_climo.attrs['naming_authority']\n",
    "    del ds_climo.attrs['product_id']\n",
    "    del ds_climo.attrs['product_name']\n",
    "    del ds_climo.attrs['algorithm']\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    #ISO 8601 time attributes are not straightforward for a monthly climalogy period. We remove them.\n",
    "    del ds_climo.attrs['time_coverage_start'] \n",
    "    del ds_climo.attrs['time_coverage_end']\n",
    "    del ds_climo.attrs['time_coverage_duration']\n",
    "    del ds_climo.attrs['time_coverage_resolution']\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2f2559c-2852-4047-a27d-ecf59d6ff9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding\n",
    "encoding = {'time': {'dtype':dso['time'].dtype, 'units':dso['time'].units, 'calendar':dso['time'].calendar,'_FillValue':None},\n",
    "            'climatology_bnds': {'dtype':dso['time'].dtype, 'units':dso['time'].units, 'calendar':dso['time'].calendar, '_FillValue':None},\n",
    "            'lat': {'_FillValue':None}, 'lon': {'_FillValue':None},\n",
    "            'xc': {'_FillValue':None}, 'yc': {'_FillValue':None},\n",
    "            'xc_bnds': {'_FillValue':None}, 'yc_bnds': {'_FillValue':None},\n",
    "            'Lambert_Azimuthal_Grid': {'_FillValue':None, 'dtype':dso['Lambert_Azimuthal_Grid'].dtype}\n",
    "           }\n",
    "climo_vars = []\n",
    "for v in ds_climo.variables:\n",
    "    if v.startswith('ice_conc_'):\n",
    "        encoding[v] = {'dtype':np.float32, '_FillValue':np.float32(-999.)}\n",
    "        climo_vars.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b326f477-6050-4cb7-864f-0a64e88e67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder the variables in the dataset before writing to disk.\n",
    "ordered_variables = ['Lambert_Azimuthal_Grid', 'time', 'climatology_bnds',\n",
    "                     'xc', 'yc', 'xc_bnds', 'yc_bnds', 'lat', 'lon',] + climo_vars\n",
    "if len(ordered_variables) != len(ds_climo.variables):\n",
    "    raise ValueError(\"Missing some variables!\")\n",
    "\n",
    "\n",
    "ds_climo_2 = xr.Dataset({var_name: ds_climo[var_name] for var_name in ordered_variables}, attrs=ds_climo.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a019efed-e710-43ba-a28a-5ab41c12f1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:                 (time: 12, nv: 2, xc: 432, yc: 432)\n",
      "Coordinates:\n",
      "  * time                    (time) object 1996-01-16 00:00:00 ... 1996-12-16 ...\n",
      "  * xc                      (xc) float64 -5.388e+03 -5.362e+03 ... 5.388e+03\n",
      "  * yc                      (yc) float64 5.388e+03 5.362e+03 ... -5.388e+03\n",
      "    lon                     (yc, xc) float32 -135.0 -135.1 -135.3 ... 44.87 45.0\n",
      "    lat                     (yc, xc) float32 16.62 16.82 17.02 ... 16.82 16.62\n",
      "Dimensions without coordinates: nv\n",
      "Data variables: (12/15)\n",
      "    Lambert_Azimuthal_Grid  int32 ...\n",
      "    climatology_bnds        (time, nv) object 1991-01-01 00:00:00 ... 2001-01...\n",
      "    xc_bnds                 (xc, nv) float64 -5.4e+03 -5.375e+03 ... 5.4e+03\n",
      "    yc_bnds                 (yc, nv) float64 5.4e+03 5.375e+03 ... -5.4e+03\n",
      "    ice_conc_mean           (time, yc, xc) float64 dask.array<chunksize=(1, 432, 432), meta=np.ndarray>\n",
      "    ice_conc_sdev           (time, yc, xc) float64 dask.array<chunksize=(1, 432, 432), meta=np.ndarray>\n",
      "    ...                      ...\n",
      "    ice_conc_5pctile        (time, yc, xc) float64 dask.array<chunksize=(1, 432, 432), meta=np.ndarray>\n",
      "    ice_conc_10pctile       (time, yc, xc) float64 dask.array<chunksize=(1, 432, 432), meta=np.ndarray>\n",
      "    ice_conc_25pctile       (time, yc, xc) float64 dask.array<chunksize=(1, 432, 432), meta=np.ndarray>\n",
      "    ice_conc_75pctile       (time, yc, xc) float64 dask.array<chunksize=(1, 432, 432), meta=np.ndarray>\n",
      "    ice_conc_90pctile       (time, yc, xc) float64 dask.array<chunksize=(1, 432, 432), meta=np.ndarray>\n",
      "    ice_conc_95pctile       (time, yc, xc) float64 dask.array<chunksize=(1, 432, 432), meta=np.ndarray>\n",
      "Attributes: (12/34)\n",
      "    title:                     Monthly Climatology (1991-2000) of Sea Ice Con...\n",
      "    summary:                   This climate data record of sea ice concentrat...\n",
      "    topiccategory:             Oceans ClimatologyMeteorologyAtmosphere\n",
      "    geospatial_lat_min:        16.62393\n",
      "    geospatial_lat_max:        90.0\n",
      "    geospatial_lon_min:        -180.0\n",
      "    ...                        ...\n",
      "    Conventions:               CF-1.7,ACDD-1.3\n",
      "    references:                Product User Manual v3 (2022),Algorithm Theore...\n",
      "    contributor_name:          Thomas Lavergne,Atle Soerensen,Rasmus Tonboe,C...\n",
      "    contributor_role:          PrincipalInvestigator,author,author,author,aut...\n",
      "    source:                    FCDR of SMMR / SSMI / SSMIS Brightness Tempera...\n",
      "    product_status:            under development\n"
     ]
    }
   ],
   "source": [
    "print(ds_climo_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ebafc31-2c63-4e2e-973d-27128788e956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomasl/mambaforge/envs/py310/lib/python3.10/site-packages/dask/array/numpy_compat.py:41: RuntimeWarning: invalid value encountered in divide\n",
      "  x = np.divide(x1, x2, out)\n",
      "/home/thomasl/mambaforge/envs/py310/lib/python3.10/site-packages/numpy/lib/nanfunctions.py:1577: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanquantile_1d, axis, a, q,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ice_conc_nh_ease2-250_climatology-v3p0_1991-2000.nc is ready.\n"
     ]
    }
   ],
   "source": [
    "# write to netCDF/CF\n",
    "\n",
    "## Note : this filename is still just a proposal\n",
    "outname = './ice_conc_{}_ease2-250_climatology-v3p0_{}-{}.nc'.format(area, climo[0], climo[1])\n",
    "\n",
    "ds_climo_2.to_netcdf(outname, encoding=encoding)\n",
    "\n",
    "print(outname, \"is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5c1c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
